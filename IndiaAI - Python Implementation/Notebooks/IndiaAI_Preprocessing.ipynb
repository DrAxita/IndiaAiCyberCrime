{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815739f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train_utf8.csv')\n",
    "test_df = pd.read_csv('test_utf8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce8be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Missing Values:\n",
      "category              0\n",
      "sub_category          0\n",
      "crimeaditionalinfo    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing 'crimeaditionalinfo'\n",
    "train_df = train_df.dropna(subset=['crimeaditionalinfo'])\n",
    "\n",
    "# Fill missing 'sub_category' with a placeholder (e.g., 'Unknown')\n",
    "train_df['sub_category'] = train_df['sub_category'].fillna('Unknown')\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Updated Missing Values:\")\n",
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6bc85",
   "metadata": {},
   "source": [
    "### Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635fc975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 19:05:35,383: E symspellpy.symspellpy] Dictionary file not found at frequency_dictionary_en_82_765.txt.\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 93665/93665 [05:29<00:00, 284.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 31229/31229 [01:49<00:00, 285.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sample Complaints:\n",
      "                                  crimeaditionalinfo  \\\n",
      "0  I had continue received random calls and abusi...   \n",
      "1  The above fraudster is continuously messaging ...   \n",
      "2  He is acting like a police and demanding for m...   \n",
      "3  In apna Job I have applied for job interview f...   \n",
      "4  I received a call from lady stating that she w...   \n",
      "\n",
      "                                      corrected_text  \n",
      "0  i had continue received random calls and abusi...  \n",
      "1  the above fraudster is continuously messaging ...  \n",
      "2  he is acting like a police and demanding for m...  \n",
      "3  in apna job i have applied for job interview f...  \n",
      "4  i received a call from lady stating that she w...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SymSpell\n",
    "max_edit_distance_dictionary = 2  # Maximum edit distance for lookups\n",
    "prefix_length = 7  # Length of prefixes used for dictionary entries\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "# Load a dictionary file (pre-built dictionary for faster processing)\n",
    "# Download frequency_dictionary_en_82_765.txt from https://github.com/mammothb/symspellpy\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "term_index = 0  # Column of the term in the dictionary\n",
    "count_index = 1  # Column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index, count_index)\n",
    "\n",
    "# Function for spelling correction using SymSpell\n",
    "def correct_spelling(text):\n",
    "    if isinstance(text, str):\n",
    "        suggestions = sym_spell.lookup_compound(text, max_edit_distance_dictionary)\n",
    "        if suggestions:\n",
    "            return suggestions[0].term  # Return the best suggestion\n",
    "    return text  # Return as-is for non-string inputs\n",
    "\n",
    "# Use tqdm to show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Example DataFrames (replace with your actual data)\n",
    "# train_df = ...\n",
    "# test_df = ...\n",
    "\n",
    "# Apply spelling correction with progress bar\n",
    "train_df['corrected_text'] = train_df['crimeaditionalinfo'].progress_apply(correct_spelling)\n",
    "test_df['corrected_text'] = test_df['crimeaditionalinfo'].progress_apply(correct_spelling)\n",
    "\n",
    "# Check corrected text\n",
    "print(\"Corrected Sample Complaints:\")\n",
    "print(train_df[['crimeaditionalinfo', 'corrected_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da29b8",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a8ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 93665/93665 [00:54<00:00, 1721.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 31229/31229 [00:18<00:00, 1674.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sample Complaints:\n",
      "                                      corrected_text  \\\n",
      "0  i had continue received random calls and abusi...   \n",
      "1  the above fraudster is continuously messaging ...   \n",
      "2  he is acting like a police and demanding for m...   \n",
      "3  in apna job i have applied for job interview f...   \n",
      "4  i received a call from lady stating that she w...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [i, had, continue, received, random, calls, an...  \n",
      "1  [the, above, fraudster, is, continuously, mess...  \n",
      "2  [he, is, acting, like, a, police, and, demandi...  \n",
      "3  [in, apna, job, i, have, applied, for, job, in...  \n",
      "4  [i, received, a, call, from, lady, stating, th...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download tokenizer if needed\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if isinstance(text, str) else []\n",
    "\n",
    "# Use tqdm to show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "train_df['tokens'] = train_df['corrected_text'].progress_apply(tokenize_text)\n",
    "test_df['tokens'] = test_df['corrected_text'].progress_apply(tokenize_text)\n",
    "\n",
    "# Check tokenized complaints\n",
    "print(\"Tokenized Sample Complaints:\")\n",
    "print(train_df[['corrected_text', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db9fb0",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c44c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 93665/93665 [00:52<00:00, 1794.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 31229/31229 [00:17<00:00, 1828.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sample Complaints:\n",
      "                                      corrected_text  \\\n",
      "0  i had continue received random calls and abusi...   \n",
      "1  the above fraudster is continuously messaging ...   \n",
      "2  he is acting like a police and demanding for m...   \n",
      "3  in apna job i have applied for job interview f...   \n",
      "4  i received a call from lady stating that she w...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [i, had, continue, received, random, calls, an...  \n",
      "1  [the, above, fraudster, is, continuously, mess...  \n",
      "2  [he, is, acting, like, a, police, and, demandi...  \n",
      "3  [in, apna, job, i, have, applied, for, job, in...  \n",
      "4  [i, received, a, call, from, lady, stating, th...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download tokenizer if needed\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if isinstance(text, str) else []\n",
    "\n",
    "# Use tqdm to show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply tokenization with progress bar\n",
    "train_df['tokens'] = train_df['corrected_text'].progress_apply(tokenize_text)\n",
    "test_df['tokens'] = test_df['corrected_text'].progress_apply(tokenize_text)\n",
    "\n",
    "# Check tokenized complaints\n",
    "print(\"Tokenized Sample Complaints:\")\n",
    "print(train_df[['corrected_text', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9e4bae",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0376dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS-Tagged Sample Complaints:\n",
      "                                              tokens  \\\n",
      "0  [i, had, continue, received, random, calls, an...   \n",
      "1  [the, above, fraudster, is, continuously, mess...   \n",
      "2  [he, is, acting, like, a, police, and, demandi...   \n",
      "3  [in, apna, job, i, have, applied, for, job, in...   \n",
      "4  [i, received, a, call, from, lady, stating, th...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(i, NN), (had, VBD), (continue, VBN), (receiv...  \n",
      "1  [(the, DT), (above, JJ), (fraudster, NN), (is,...  \n",
      "2  [(he, PRP), (is, VBZ), (acting, VBG), (like, I...  \n",
      "3  [(in, IN), (apna, JJ), (job, NN), (i, NNS), (h...  \n",
      "4  [(i, NN), (received, VBD), (a, DT), (call, NN)...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from tqdm import tqdm\n",
    "# Function for POS tagging\n",
    "def pos_tag_tokens(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "tqdm.pandas()\n",
    "# Apply POS tagging to create 'pos_tags' column\n",
    "train_df['pos_tags'] = train_df['tokens'].apply(pos_tag_tokens)\n",
    "test_df['pos_tags'] = test_df['tokens'].apply(pos_tag_tokens)\n",
    "\n",
    "# Check POS tagging results\n",
    "print(\"POS-Tagged Sample Complaints:\")\n",
    "print(train_df[['tokens', 'pos_tags']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3c3de",
   "metadata": {},
   "source": [
    "### Stop Word Removal Based on Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "707e7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens Based on POS Tags (Sample):\n",
      "                                            pos_tags  \\\n",
      "0  [(i, NN), (had, VBD), (continue, VBN), (receiv...   \n",
      "1  [(the, DT), (above, JJ), (fraudster, NN), (is,...   \n",
      "2  [(he, PRP), (is, VBZ), (acting, VBG), (like, I...   \n",
      "3  [(in, IN), (apna, JJ), (job, NN), (i, NNS), (h...   \n",
      "4  [(i, NN), (received, VBD), (a, DT), (call, NN)...   \n",
      "\n",
      "                                     filtered_tokens  \n",
      "0  [i, had, continue, received, random, calls, ab...  \n",
      "1  [above, fraudster, is, continuously, messaging...  \n",
      "2  [is, acting, police, demanding, money, adding,...  \n",
      "3  [apna, job, i, have, applied, job, interview, ...  \n",
      "4  [i, received, call, lady, stating, send, new, ...  \n"
     ]
    }
   ],
   "source": [
    "# Define POS tags to keep\n",
    "important_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
    "\n",
    "# Function to filter tokens based on POS tags\n",
    "def filter_by_pos(pos_tags):\n",
    "    return [word for word, tag in pos_tags if tag in important_pos_tags]\n",
    "tqdm.pandas()\n",
    "# Apply the filtering\n",
    "train_df['filtered_tokens'] = train_df['pos_tags'].apply(filter_by_pos)\n",
    "test_df['filtered_tokens'] = test_df['pos_tags'].apply(filter_by_pos)\n",
    "\n",
    "# Check filtered tokens\n",
    "print(\"Filtered Tokens Based on POS Tags (Sample):\")\n",
    "print(train_df[['pos_tags', 'filtered_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba7eb8",
   "metadata": {},
   "source": [
    "### Domain-Based Entity Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "277501df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain-Based Entity Tags (Sample):\n",
      "                                     filtered_tokens  \\\n",
      "0  [i, had, continue, received, random, calls, ab...   \n",
      "1  [above, fraudster, is, continuously, messaging...   \n",
      "2  [is, acting, police, demanding, money, adding,...   \n",
      "3  [apna, job, i, have, applied, job, interview, ...   \n",
      "4  [i, received, call, lady, stating, send, new, ...   \n",
      "\n",
      "                                         domain_tags  \n",
      "0  [(i, O), (had, O), (continue, O), (received, O...  \n",
      "1  [(above, O), (fraudster, O), (is, O), (continu...  \n",
      "2  [(is, O), (acting, O), (police, O), (demanding...  \n",
      "3  [(apna, O), (job, Online Job Fraud), (i, O), (...  \n",
      "4  [(i, O), (received, O), (call, Fraud CallVishi...  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Use tqdm to show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# Domain Entities Dictionary\n",
    "domain_entities = {\n",
    "    'Business Email CompromiseEmail Takeover': ['email', 'takeover', 'business'],\n",
    "    'Cheating by Impersonation': ['cheating', 'impersonation', 'fraud'],\n",
    "    'Cryptocurrency Fraud': ['cryptocurrency', 'bitcoin', 'crypto', 'fraud'],\n",
    "    'Cyber Bullying  Stalking  Sexting': ['bullying', 'stalking', 'sexting', 'harassment'],\n",
    "    'Cyber Terrorism': ['terrorism', 'cyber', 'extremism'],\n",
    "    'Damage to computer computer systems etc': ['damage', 'computer', 'systems'],\n",
    "    'Data Breach/Theft': ['data', 'breach', 'theft', 'hacking'],\n",
    "    'DebitCredit Card FraudSim Swap Fraud': ['card', 'debit', 'credit', 'fraud', 'sim'],\n",
    "    'Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks': ['ddos', 'dos', 'denial'],\n",
    "    'EWallet Related Fraud': ['ewallet', 'fraud', 'wallet', 'money'],\n",
    "    'Email Hacking': ['email', 'hacking', 'phishing'],\n",
    "    'FakeImpersonating Profile': ['fake', 'impersonating', 'profile'],\n",
    "    'Fraud CallVishing': ['fraud', 'call', 'vishing'],\n",
    "    'Hacking/Defacement': ['hacking', 'defacement'],\n",
    "    'Online Gambling  Betting': ['gambling', 'betting', 'online'],\n",
    "    'Online Job Fraud': ['job', 'fraud', 'scam'],\n",
    "    'Online Matrimonial Fraud': ['matrimonial', 'marriage', 'fraud'],\n",
    "    'Ransomware': ['ransomware', 'attack', 'encryption'],\n",
    "    'UPI Related Frauds': ['upi', 'fraud', 'transaction', 'money'],\n",
    "    'Website DefacementHacking': ['website', 'hacking', 'defacement']\n",
    "}\n",
    "\n",
    "# Flatten the dictionary for NER tagging\n",
    "entity_mapping = {word: sub_category for sub_category, words in domain_entities.items() for word in words}\n",
    "\n",
    "# Tagging Function\n",
    "def tag_domain_entities(tokens):\n",
    "    return [(token, entity_mapping.get(token.lower(), 'O')) for token in tokens]\n",
    "\n",
    "# Parallel Apply Function\n",
    "def parallel_apply(df, func, column):\n",
    "    return Parallel(n_jobs=-1)(delayed(func)(tokens) for tokens in df[column])\n",
    "\n",
    "# Toggle for Method Selection\n",
    "use_parallel = True  # Set to False to use tqdm with pandas\n",
    "\n",
    "if use_parallel:\n",
    "    # Apply tagging in parallel\n",
    "    train_df['domain_tags'] = parallel_apply(train_df, tag_domain_entities, 'filtered_tokens')\n",
    "    test_df['domain_tags'] = parallel_apply(test_df, tag_domain_entities, 'filtered_tokens')\n",
    "else:\n",
    "    # Apply tagging with tqdm progress bar\n",
    "    train_df['domain_tags'] = train_df['filtered_tokens'].progress_apply(tag_domain_entities)\n",
    "    test_df['domain_tags'] = test_df['filtered_tokens'].progress_apply(tag_domain_entities)\n",
    "\n",
    "# Check domain tagging\n",
    "print(\"Domain-Based Entity Tags (Sample):\")\n",
    "print(train_df[['filtered_tokens', 'domain_tags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54017b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved as CSV files:\n",
      "- Processed training data: processed_train.csv\n",
      "- Processed test data: processed_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the processed training data to a CSV file\n",
    "train_df.to_csv('processed_train.csv', index=False)\n",
    "\n",
    "# Save the processed test data to a CSV file\n",
    "test_df.to_csv('processed_test.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved as CSV files:\")\n",
    "print(\"- Processed training data: processed_train.csv\")\n",
    "print(\"- Processed test data: processed_test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
